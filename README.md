# Integrating Gemma into RAG Pipelines ğŸš€

Bu proje, Google'Ä±n **Gemma** modellerini yerel ortamda (Ollama kullanarak) Ã§alÄ±ÅŸtÄ±rÄ±p, **LangChain** ve **ChromaDB** kullanarak basit bir RAG (Retrieval-Augmented Generation) hattÄ±nÄ±n nasÄ±l kurulacaÄŸÄ±nÄ± gÃ¶steren bir demodur.

Ã–zellikle bu alana yeni baÅŸlayanlara; standart bir LLM cevabÄ± ile RAG destekli cevap arasÄ±ndaki farkÄ± somut bir ÅŸekilde gÃ¶stermeyi amaÃ§lar.

## ğŸ“Š Sunum

Projenin teorik altyapÄ±sÄ±nÄ± ve mimari detaylarÄ±nÄ± iÃ§eren sunuma aÅŸaÄŸÄ±daki baÄŸlantÄ±dan ulaÅŸabilirsiniz:

ğŸ‘‰ **[Canva Sunumu: Integrating Gemma into RAG Pipelines](https://www.canva.com/design/DAG5cMLunI4/UDbW1tlubl1zOw318Mfgdg/view?utm_content=DAG5cMLunI4&utm_campaign=designshare&utm_medium=link2&utm_source=uniquelinks&utlId=hf89e05b805)**

---

## ğŸ› ï¸ Kurulum ve Ã–n HazÄ±rlÄ±k

Bu projeyi Ã§alÄ±ÅŸtÄ±rmak iÃ§in bilgisayarÄ±nÄ±zda **Python** ve **Ollama** kurulu olmalÄ±dÄ±r.

### 1. Ollama Kurulumu ve Modellerin Ã‡ekilmesi
Ã–ncelikle [Ollama.com](https://ollama.com/) adresinden uygulamanÄ±n iÅŸletim sisteminize uygun sÃ¼rÃ¼mÃ¼nÃ¼ indirin. ArdÄ±ndan terminal Ã¼zerinden bu projede kullanÄ±lan LLM (`gemma3:4b`) ve Embedding (`nomic-embed-text`) modellerini indirin:

```bash
ollama pull gemma3:4b
ollama pull nomic-embed-text
